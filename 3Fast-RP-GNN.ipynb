{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast-RP-GNN, Relational Pooling, Mini-batching, Subgraph batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Reddit, PPI, Planetoid\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch_geometric.utils.repeat import repeat\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "import time\n",
    "from torch_cluster import neighbor_sampler\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import degree, segregate_self_loops\n",
    "from torch_geometric.utils.repeat import repeat\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "\n",
    "\n",
    "class NeighborSamplerNew(object):\n",
    "    \n",
    "    def __init__(self, data, size=0, num_hops=2, batch_size=1, shuffle=False,\n",
    "                 drop_last=False, bipartite=False, add_self_loops=False,\n",
    "                 flow='source_to_target'):\n",
    "\n",
    "        self.data = data\n",
    "        self.size = repeat(k, hop)\n",
    "        self.num_hops = hop\n",
    "        self.batch_size = neighbor_minibatch_size\n",
    "        self.shuffle = False\n",
    "        self.drop_last = False\n",
    "        self.bipartite = False\n",
    "        self.add_self_loops = False\n",
    "        self.flow = 'source_to_target'\n",
    "\n",
    "        self.edge_index = data.edge_index\n",
    "        self.e_id = torch.arange(self.edge_index.size(1))\n",
    "        if bipartite and add_self_loops:\n",
    "            tmp = segregate_self_loops(self.edge_index, self.e_id)\n",
    "            self.edge_index, self.e_id, self.edge_index_loop = tmp[:3]\n",
    "            self.e_id_loop = self.e_id.new_full((data.num_nodes, ), -1)\n",
    "            self.e_id_loop[tmp[2][0]] = tmp[3]\n",
    "\n",
    "        assert flow in ['source_to_target', 'target_to_source']\n",
    "        self.i, self.j = (0, 1) if flow == 'target_to_source' else (1, 0)\n",
    "\n",
    "        edge_index_i, self.e_assoc = self.edge_index[self.i].sort()\n",
    "        self.edge_index_j = self.edge_index[self.j, self.e_assoc]\n",
    "        deg = degree(edge_index_i, data.num_nodes, dtype=torch.long)\n",
    "        self.cumdeg = torch.cat([deg.new_zeros(1), deg.cumsum(0)])\n",
    "\n",
    "        self.tmp = torch.empty(data.num_nodes, dtype=torch.long)\n",
    "        \n",
    "\n",
    "\n",
    "    def __get_batches__(self, subset=None):\n",
    "        r\"\"\"Returns a list of mini-batches from the initial nodes in\n",
    "        :obj:`subset`.\"\"\"\n",
    "\n",
    "        if subset is None and not self.shuffle:\n",
    "            subset = torch.arange(self.data.num_nodes, dtype=torch.long)\n",
    "        elif subset is None and self.shuffle:\n",
    "            subset = torch.randperm(self.data.num_nodes)\n",
    "        else:\n",
    "            if subset.dtype == torch.bool or subset.dtype == torch.uint8:\n",
    "                subset = subset.nonzero().view(-1)\n",
    "            if self.shuffle:\n",
    "                subset = subset[torch.randperm(subset.size(0))]\n",
    "\n",
    "        subsets = torch.split(subset, self.batch_size)\n",
    "        if self.drop_last and subsets[-1].size(0) < self.batch_size:\n",
    "            subsets = subsets[:-1]\n",
    "        assert len(subsets) > 0\n",
    "        return subsets\n",
    "\n",
    "  \n",
    "    def __produce_subgraph__(self, b_id):\n",
    "        r\"\"\"Produces a :obj:`Data` object holding the subgraph data for a given\n",
    "        mini-batch :obj:`b_id`.\"\"\"\n",
    "\n",
    "        n_ids = [b_id]\n",
    "        e_ids = []\n",
    "        edge_indices = []\n",
    "\n",
    "        for l in range(self.num_hops):\n",
    "            e_id = neighbor_sampler(n_ids[-1], self.cumdeg, self.size[l])\n",
    "            n_id = self.edge_index_j.index_select(0, e_id)\n",
    "            n_id = n_id.unique(sorted=False)\n",
    "            n_ids.append(n_id)\n",
    "            e_ids.append(self.e_assoc.index_select(0, e_id))\n",
    "            edge_index = self.data.edge_index.index_select(1, e_ids[-1])\n",
    "            edge_indices.append(edge_index)\n",
    "\n",
    "        n_id = torch.unique(torch.cat(n_ids, dim=0), sorted=False)\n",
    "        self.tmp[n_id] = torch.arange(n_id.size(0))\n",
    "        e_id = torch.cat(e_ids, dim=0)\n",
    "        edge_index = self.tmp[torch.cat(edge_indices, dim=1)]\n",
    "\n",
    "        num_nodes = n_id.size(0)\n",
    "        idx = edge_index[0] * num_nodes + edge_index[1]\n",
    "        idx, inv = idx.unique(sorted=False, return_inverse=True)\n",
    "        edge_index = torch.stack([idx / num_nodes, idx % num_nodes], dim=0)\n",
    "        e_id = e_id.new_zeros(edge_index.size(1)).scatter_(0, inv, e_id)\n",
    "\n",
    "        return Data(edge_index=edge_index, e_id=e_id, n_id=n_id, b_id=b_id,\n",
    "                    sub_b_id=self.tmp[b_id], num_nodes=num_nodes)\n",
    "\n",
    "    def __call__(self, subset=None):\n",
    "        r\"\"\"Returns a generator of :obj:`DataFlow` that iterates over the nodes\n",
    "        in :obj:`subset` in a mini-batch fashion.\n",
    "        Args:\n",
    "            subset (LongTensor or BoolTensor, optional): The initial nodes to\n",
    "                propagete messages to. If set to :obj:`None`, will iterate over\n",
    "                all nodes in the graph. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        if self.bipartite:\n",
    "            produce = self.__produce_bipartite_data_flow__\n",
    "        else:\n",
    "            produce = self.__produce_subgraph__\n",
    "\n",
    "        for n_id in self.__get_batches__(subset):\n",
    "            yield produce(n_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset, the type of prediction and the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cora'\n",
    "PREDICTION = 'link'\n",
    "RUN_COUNT = 1\n",
    "NUM_SAMPLES = 1\n",
    "PATH_TO_DATASETS_DIRECTORY = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'reddit': Reddit(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Reddit'),\n",
    "    'cora' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Cora/', name='Cora'),\n",
    "    'citeseer' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/CiteSeer/', name='CiteSeer'),\n",
    "    'pubmed' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/PubMed/', name='PubMed'),\n",
    "}\n",
    "dataset = datasets[DATASET]\n",
    "data = dataset[0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Dataset Characteristics\n",
      "Name:  cora\n",
      "Total Number of Nodes:  2708\n",
      "Total Number of Training Nodes:  140\n",
      "Total Number of Val Nodes:  500\n",
      "Total Number of Test Nodes:  1000\n",
      "Num Node Features:  1433\n",
      "Num Node Classes:  7\n",
      "Number of Edges:  10556\n",
      "Number of Samples for structural:  1\n",
      "Prediction Type:  link\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing Dataset Characteristics\")\n",
    "print(\"Name: \", DATASET)\n",
    "print(\"Total Number of Nodes: \", data.num_nodes)\n",
    "print(\"Total Number of Training Nodes: \", data.train_mask.sum().item())\n",
    "print(\"Total Number of Val Nodes: \", data.val_mask.sum().item())\n",
    "print(\"Total Number of Test Nodes: \", data.test_mask.sum().item())\n",
    "print(\"Num Node Features: \", data.num_features)\n",
    "print(\"Num Node Classes: \", dataset.num_classes)\n",
    "print(\"Number of Edges: \", data.edge_index.shape[1])\n",
    "print(\"Number of Samples for structural: \", NUM_SAMPLES)\n",
    "print(\"Prediction Type: \", PREDICTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.train_mask = 1 - data.val_mask - data.test_mask\n",
    "data.train_mask = ~(data.val_mask + data.test_mask)\n",
    "\n",
    "adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "edges = data.edge_index.t()\n",
    "adj_mat[edges[:,0], edges[:,1]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the non-overlapping induced subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train = adj_mat[data.train_mask].t()[data.train_mask].t()\n",
    "adj_validation = adj_mat[data.val_mask].t()[data.val_mask].t()\n",
    "adj_test = adj_mat[data.test_mask].t()[data.test_mask].t()\n",
    "\n",
    "\n",
    "GnnLayer=GATConv\n",
    "gnn_model = 0\n",
    "epochs = 60\n",
    "validation_acc = 0\n",
    "small_samples = 200\n",
    "\n",
    "minibatch_size=16\n",
    "neighbor_minibatch_size=2 #for current implementation use 2 for now\n",
    "k=5\n",
    "hop=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrupt a small fraction of the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_adj(adj_mat, task, percent=2):\n",
    "    \"\"\" Returns the corrupted version of the adjacency matrix \"\"\"\n",
    "    if task == 'link':\n",
    "        edges = adj_mat.triu().nonzero()\n",
    "        num_edges = edges.shape[0]\n",
    "        num_to_corrupt = int(percent/100.0 * num_edges)\n",
    "        random_corruption = np.random.randint(num_edges, size=num_to_corrupt)\n",
    "        adj_mat_corrupted = adj_mat.clone()\n",
    "        false_edges, false_non_edges = [], []\n",
    "        #Edge Corruption\n",
    "        for ed in edges[random_corruption]:\n",
    "            adj_mat_corrupted[ed[0], ed[1]] = 0\n",
    "            adj_mat_corrupted[ed[1], ed[0]] = 0\n",
    "            false_non_edges.append(ed.tolist())\n",
    "        #Non Edge Corruption\n",
    "        random_non_edge_corruption = list(np.random.randint(adj_mat.shape[0], size = 6*num_to_corrupt))\n",
    "        non_edge_to_corrupt = []\n",
    "        for k in range(len(random_non_edge_corruption)-1):\n",
    "            to_check = [random_non_edge_corruption[k], random_non_edge_corruption[k+1]]\n",
    "            if to_check not in edges.tolist():\n",
    "                non_edge_to_corrupt.append(to_check)\n",
    "            if len(non_edge_to_corrupt) == num_to_corrupt:\n",
    "                break\n",
    "        non_edge_to_corrupt = torch.Tensor(non_edge_to_corrupt).type(torch.int16)\n",
    "        for n_ed in non_edge_to_corrupt:\n",
    "            adj_mat_corrupted[n_ed[0], n_ed[1]] = 1\n",
    "            adj_mat_corrupted[n_ed[1], n_ed[0]] = 1\n",
    "            false_edges.append(n_ed.tolist())\n",
    "    return adj_mat_corrupted, false_edges, false_non_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train edge index: tensor([[   1,    1,    1,  ..., 1205, 1205, 1207],\n",
      "        [   2,  152,  154,  ...,  789, 1124,  941]])\n"
     ]
    }
   ],
   "source": [
    "adj_train_corrupted, train_false_edges, train_false_non_edges = corrupt_adj(adj_train, 'link', percent=2)\n",
    "adj_val_corrupted, val_false_edges, val_false_non_edges = corrupt_adj(adj_validation, 'link', percent=2)\n",
    "adj_test_corrupted, test_false_edges, test_false_non_edges  = corrupt_adj(adj_test, 'link', percent=2)\n",
    "\n",
    "G_train=Data(edge_index=(adj_train_corrupted.nonzero()).t(), x=data.x[data.train_mask])\n",
    "print(\"train edge index:\", G_train.edge_index)\n",
    "G_val=Data(edge_index=(adj_val_corrupted.nonzero()).t(), x=data.x[data.val_mask])\n",
    "G_test=Data(edge_index=(adj_test_corrupted.nonzero()).t(), x=data.x[data.test_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the GNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nneurons = 256\n",
    "Input_Dim_rep = data.num_features + 1 #aditional feature for expressiveness of the Graph Convolution\n",
    "\n",
    "class GnnNew(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GnnNew, self).__init__()        \n",
    "     \n",
    "        if(GnnLayer==GINConv):\n",
    "            self.MLP1 = nn.Linear(Input_Dim_rep,Nneurons)\n",
    "            self.MLP2 = nn.Linear(Nneurons,Nneurons)\n",
    "            self.GNN_layer1 = GnnLayer(self.MLP1)\n",
    "            self.GNN_layer2 = GnnLayer(self.MLP2)  \n",
    "        \n",
    "        else:\n",
    "            self.GNN_layer1 = GnnLayer(Input_Dim_rep,Nneurons)\n",
    "            self.GNN_layer2 = GnnLayer(Nneurons,Nneurons)             \n",
    "    \n",
    "        self.ds_layer_1 = nn.Linear(Nneurons, Nneurons)\n",
    "        self.ro_1 = nn.Linear(Nneurons, Nneurons)      \n",
    "              \n",
    "        self.linear_1 = nn.Linear(Nneurons,Nneurons)\n",
    "        self.linear_2 = nn.Linear(Nneurons,2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, subgraphs, nodes, edges):        \n",
    "        \n",
    "        \n",
    "        countNode = torch.zeros((len(nodes)),requires_grad=False).to(device)\n",
    "        tensorNode = torch.zeros((len(nodes), Nneurons)).to(device)\n",
    "        \n",
    "        for i in range(len(subgraphs)):\n",
    "        \n",
    "            graph=subgraphs[i]\n",
    "            feat=graph.x_feature\n",
    "            lenFeat=len(feat)\n",
    "            ed_i=graph.edge_index\n",
    "            Nfeat=feat[0].shape[0]\n",
    "            \n",
    "            Ysum = torch.zeros((Nfeat, Nneurons)).to(device)\n",
    "            \n",
    "            for j in range(lenFeat):           \n",
    "      \n",
    "                y=self.GNN_layer1(feat[j],ed_i)\n",
    "                y=self.relu(y)\n",
    "\n",
    "                y=self.GNN_layer2(y,ed_i)\n",
    "                y=self.relu(y)       \n",
    "                Ysum=Ysum+y\n",
    "            \n",
    "            Yavg=Ysum/lenFeat\n",
    "            \n",
    "            for node_i_d in graph.b_id:             \n",
    "                # b_id holds a one-dimensional tensor of node indices to produce subgraphs \n",
    "                #for and e_id and n_id hold the indices that got samples from the original graph for edges \n",
    "                #and nodes respectively. Since you are having a batch size of 1, b_id should be of size 1.\n",
    "#                 print(\"graph.nid\", graph.n_id)\n",
    "#                 print(\"nodeId\", node_i_d)\n",
    "#                 print(\"graph.nid=nodeId\", (graph.n_id==node_i_d))\n",
    "#                 print(\"subindex\", (graph.n_id==node_i_d).nonzero()[0])\n",
    "                sub_index=0\n",
    "                for i_d in range(len(graph.n_id)):\n",
    "                    if graph.n_id[i_d]==node_i_d:\n",
    "                        sub_index=i_d\n",
    "                        break;\n",
    "                #print(\"subindexnew\", sub_index)\n",
    "                #sub_index=(graph.n_id==node_i_d).nonzero()[0]\n",
    "                sub_x=Yavg[sub_index,:]\n",
    "                node_index=0\n",
    "                for i_d in range(len(nodes)):\n",
    "                    if nodes[i_d]==node_i_d:\n",
    "                        node_index=i_d\n",
    "                        break;\n",
    "                \n",
    "                #node_index=(nodes==node_i_d).nonzero()[0]\n",
    "#                 print(\"node.nid\", nodes)\n",
    "#                 print(\"nodeId\", node_i_d)\n",
    "#                 print(\"nodes=nodeId\", (nodes==node_i_d))\n",
    "#                 print(\"nodeindex\", (nodes==node_i_d).nonzero()[0], \"actual Nid: \",node_index)                \n",
    "#                 print(sub_x.shape)\n",
    "#                 print(sub_x.view(-1).shape)\n",
    "                tensorNode[node_index,:]+=sub_x.view(-1)\n",
    "                countNode[node_index]+=1\n",
    "#         print(tensorNode)\n",
    "#         print(countNode)\n",
    "#         print(countNode[:, None])\n",
    "        #print(countNode.unsqueeze(0))\n",
    "        #print(countNode.unsqueeze(1))\n",
    "       \n",
    "        #tensorNode=tensorNode / countNode[:, None] \n",
    "        linkN=0\n",
    "        returnTensor = torch.zeros((len(edges), Nneurons)).to(device)\n",
    "        #print(\"retTensor\", returnTensor)\n",
    "        \n",
    "        tensorNode=tensorNode / countNode.unsqueeze(1)\n",
    "        for e in edges:\n",
    "            for i_d in range(len(nodes)):\n",
    "                    if nodes[i_d]==e[0]:\n",
    "                        point1=i_d\n",
    "                        break;\n",
    "            for i_d in range(len(nodes)):\n",
    "                    if nodes[i_d]==e[1]:\n",
    "                        point2=i_d\n",
    "                        break;\n",
    "#             u=(nodes==e[0]).nonzero()[0]\n",
    "#             v=(nodes==e[1]).nonzero()[0]\n",
    "  #          print(\"uv\",u,v,point1,point2)\n",
    "            u=tensorNode[point1,:]                        \n",
    "            v=tensorNode[point2,:]\n",
    "                        \n",
    "            ##we can use pooling                                    \n",
    "            u_v=u+v            \n",
    "            returnTensor[linkN,:]=u_v\n",
    "            linkN+=1\n",
    "        \n",
    "        \n",
    "        #One Hidden Layer for predictor        \n",
    "        returnTensor = self.linear_1(returnTensor)\n",
    "        returnTensor = self.relu(returnTensor)\n",
    "        returnTensor = self.linear_2(returnTensor)\n",
    "        \n",
    "        return returnTensor\n",
    "\n",
    "    def compute_loss(self, sub_graphs, nodes, edges, target):\n",
    "        \n",
    "        pred = self.forward(sub_graphs, nodes, edges)        \n",
    "        loss = F.cross_entropy(pred, target)                \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, sub_graphs, nodes, edges, target):\n",
    "        \n",
    "        pred = self.forward(sub_graphs, nodes, edges)        \n",
    "        loss = F.cross_entropy(pred, target)\n",
    "        \n",
    "        return loss, pred\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample positive and negative edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_equal_number_edges_non_edges(adj_mat, false_non_edges, false_edges, small_samples):\n",
    "    edges = adj_mat.nonzero()\n",
    "    num_edges = edges.shape[0]\n",
    "    inverse_adj_mat = 1 - adj_mat\n",
    "    non_edges = inverse_adj_mat.nonzero()\n",
    "    num_non_edges  = non_edges.shape[0]\n",
    "    \n",
    "    edges_sampled = edges[np.random.randint(num_edges, size=small_samples)]\n",
    "    non_edges_sampled = non_edges[np.random.randint(num_non_edges, size=small_samples)]\n",
    "    final_edges = []\n",
    "    final_non_edges = []\n",
    "    for ed in edges_sampled.tolist():\n",
    "        if ed not in false_edges:\n",
    "            final_edges.append(ed)\n",
    "    final_edges += false_non_edges\n",
    "    for n_ed in non_edges_sampled.tolist():\n",
    "        if n_ed not in false_non_edges:\n",
    "            final_non_edges.append(n_ed)\n",
    "    final_non_edges += false_edges\n",
    "\n",
    "    return final_edges, final_non_edges\n",
    "\n",
    "\n",
    "\n",
    "def sample_train_edges_nearest_neighbor(train_feature, adj_mat, false_non_edges, false_edges, small_samples):\n",
    "    edges = adj_mat.nonzero()\n",
    "    num_edges = edges.shape[0]\n",
    "    \n",
    "    k=5\n",
    "    A = kneighbors_graph(train_feature, k, mode=\"connectivity\", metric=\"cosine\", include_self=False)        \n",
    "    (u,v)=A.nonzero()\n",
    "    u=torch.Tensor(u).type(torch.long)\n",
    "    v=torch.Tensor(v).type(torch.long)\n",
    "    possible_edges=torch.stack((u,v),dim=1)\n",
    "    possible_adj_mat = torch.zeros((adj_mat.shape))\n",
    "    possible_adj_mat[possible_edges[:,0], possible_edges[:,1]] = 1\n",
    "    \n",
    "    inverse_adj_mat=possible_adj_mat-adj_mat\n",
    "    inverse_adj_mat[inverse_adj_mat==-1]=0\n",
    "    non_edges = inverse_adj_mat.nonzero()  \n",
    "    \n",
    "    print(non_edges.shape)\n",
    "    \n",
    "    num_non_edges  = non_edges.shape[0]\n",
    "    edges_sampled = edges[np.random.randint(num_edges, size=small_samples)]\n",
    "    non_edges_sampled = non_edges[np.random.randint(num_non_edges, size=small_samples)]\n",
    "    final_edges = []\n",
    "    final_non_edges = []\n",
    "    for ed in edges_sampled.tolist():\n",
    "        if ed not in false_edges:\n",
    "            final_edges.append(ed)\n",
    "    final_edges += false_non_edges\n",
    "    for n_ed in non_edges_sampled.tolist():\n",
    "        if n_ed not in false_non_edges:\n",
    "            final_non_edges.append(n_ed)\n",
    "    final_non_edges += false_edges\n",
    "\n",
    "    return final_edges, final_non_edges\n",
    "\n",
    "# edges, non_edges = sample_equal_number_edges_non_edges(adj_train_corrupted, false_non_edges=train_false_non_edges, false_edges=train_false_edges, small_samples=small_samples)\n",
    "# edges, non_edges = sample_train_edges_nearest_neighbor(G_train.x, adj_train_corrupted, false_non_edges=train_false_non_edges, false_edges=train_false_edges, small_samples=small_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatcher(object):\n",
    "    def __init__(self, batch_size, n_examples, shuffle=True):\n",
    "        assert batch_size <= n_examples, \"Error: batch_size is larger than n_examples\"\n",
    "        self.batch_size = batch_size\n",
    "        self.n_examples = n_examples\n",
    "        self.shuffle = shuffle\n",
    "        logging.info(\"batch_size={}, n_examples={}\".format(batch_size, n_examples))\n",
    "\n",
    "        self.idxs = np.arange(self.n_examples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.current_start = 0\n",
    "\n",
    "    def get_one_batch(self):\n",
    "        self.idxs = np.arange(self.n_examples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.current_start = 0\n",
    "        while self.current_start < self.n_examples:\n",
    "            batch_idxs = self.idxs[self.current_start:self.current_start+self.batch_size]\n",
    "            self.current_start += self.batch_size\n",
    "            yield torch.LongTensor(batch_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def NewSubgraphMaker(Graph, edges, nodes):    \n",
    "#     neighbor=NeighborSamplerNew(Graph)             \n",
    "#     graphGenerator=neighbor(nodes.cpu())    \n",
    "#     subgraphs=[]\n",
    "#     for graph in graphGenerator:        \n",
    "#         sub_feature=torch.Tensor(Graph.x[graph.n_id])      \n",
    "#         x_feature=[]        \n",
    "#         for i in range(2):\n",
    "#             unique_feature=torch.zeros(len(sub_feature),1)\n",
    "#             for i_d in range(len(graph.n_id)):\n",
    "#                     if graph.b_id[i]==graph.n_id[i_d]:\n",
    "#                         pointu=i_d\n",
    "#                         break;           \n",
    "#             unique_feature[pointu]=1                        \n",
    "#             x_feature.append(torch.cat((sub_feature,unique_feature),dim=1).to(device))\n",
    "#         graph.x_feature=x_feature\n",
    "#         subgraphs.append(graph.to(device))        \n",
    "#     return subgraphs\n",
    "\n",
    "def predict_model(select):\n",
    "    \n",
    "    if select==\"val\":\n",
    "        G_data=G_val\n",
    "        #neighbor_sampler=val_neighbor_sampler        \n",
    "        adj_corrupted=adj_val_corrupted\n",
    "        false_non_edges=val_false_non_edges\n",
    "        false_edges=val_false_edges\n",
    "    if select==\"test\":\n",
    "        G_data=G_test\n",
    "        #neighbor_sampler=test_neighbor_sampler        \n",
    "        adj_corrupted=adj_test_corrupted\n",
    "        false_non_edges=test_false_non_edges\n",
    "        false_edges=test_false_edges\n",
    "                \n",
    "    neighbor_sampler=NeighborSamplerNew(G_data)\n",
    "    gnn_model.eval()\n",
    "    preds=np.array([])\n",
    "    targets=np.array([])\n",
    "    total_loss=0\n",
    "    \n",
    "    edges, non_edges = sample_equal_number_edges_non_edges(adj_corrupted, false_non_edges, false_edges, small_samples)    \n",
    "    samples = torch.cat((torch.Tensor(edges), torch.Tensor(non_edges)),dim=0).type(torch.long).to(device)\n",
    "    true_target = torch.cat((torch.ones(len(edges)), torch.zeros(len(non_edges))),dim=0).type(torch.long).to(device)\n",
    "    \n",
    "    batcher = MiniBatcher(minibatch_size, len(samples)) if minibatch_size > 0 else MiniBatcher(len(samples), len(samples))\n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_idxs in batcher.get_one_batch():\n",
    "            x_idxs = x_idxs.to(device)\n",
    "\n",
    "            x_edges=samples[x_idxs]\n",
    "            y_target=true_target[x_idxs]\n",
    "            \n",
    "            nodex=torch.flatten(x_edges)            \n",
    "            \n",
    "            nodes=torch.unique(nodex.view(-1))\n",
    "\n",
    "            #subgraphs=NewSubgraphMaker(G_data, x_edges, nodex)\n",
    "                \n",
    "            graphGenerator=neighbor_sampler(nodex.cpu())    \n",
    "            subgraphs=[]\n",
    "            for graph in graphGenerator:        \n",
    "                sub_feature=torch.Tensor(G_data.x[graph.n_id])      \n",
    "                x_feature=[]        \n",
    "                for i in range(2):\n",
    "                    unique_feature=torch.zeros(len(sub_feature),1)\n",
    "                    for i_d in range(len(graph.n_id)):\n",
    "                            if graph.b_id[i]==graph.n_id[i_d]:\n",
    "                                pointu=i_d\n",
    "                                break;           \n",
    "                    unique_feature[pointu]=1                        \n",
    "                    x_feature.append(torch.cat((sub_feature,unique_feature),dim=1).to(device))\n",
    "                graph.x_feature=x_feature\n",
    "                subgraphs.append(graph.to(device))        \n",
    "            \n",
    "            loss, pred=gnn_model.predict(subgraphs, nodes, x_edges, y_target)                          \n",
    "            total_loss+=loss.item()\n",
    "            \n",
    "            pred = F.log_softmax(pred, dim=1)\n",
    "            pred = pred.detach().to(\"cpu\").numpy()\n",
    "            pred = np.argmax(pred, axis=1)\n",
    "                          \n",
    "            preds = np.append(preds,pred)\n",
    "            targets = np.append(targets,y_target.detach().to(\"cpu\").numpy())\n",
    "        \n",
    "\n",
    "    micro=f1_score(targets, preds, average='micro')\n",
    "    weighted=f1_score(targets, preds, average='weighted')\n",
    "    acc=accuracy_score(targets, preds)\n",
    "    \n",
    "    return total_loss, acc, micro, weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Accuracy and Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(train_data, valid_data, name='Loss'):\n",
    "    \"\"\"Plot\n",
    "        Plot one figure: accurace/loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_data)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_data, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, valid_data, '-', linewidth=2, label='valid')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(name)\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    plt.savefig('train_valid_'+name+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch time:  10.81801152229309\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 0] Train Loss: 19.438653826713562, Train Accuracy: 0.5011286681715575, Val Loss 17.845293045043945, Val Accuracy: 0.5024630541871922\n",
      "Minibatch time:  9.487316370010376\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Train Loss: 18.381458282470703, Train Accuracy: 0.6351351351351351, Val Loss 17.037692308425903, Val Accuracy: 0.5259259259259259\n",
      "Minibatch time:  9.10770058631897\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 2] Train Loss: 17.012055903673172, Train Accuracy: 0.6853932584269663, Val Loss 18.451271921396255, Val Accuracy: 0.5467980295566502\n"
     ]
    }
   ],
   "source": [
    "tAccurs=[]\n",
    "tLosses=[]\n",
    "vAccurs=[]\n",
    "vLosses=[]\n",
    "gnn_model = GnnNew().to(device)\n",
    "gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "gnn_best_model = 'RP_best_model.model'\n",
    "gnn_last_model = 'RP_last_model.model'\n",
    "\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "        \n",
    "    train_loss=0\n",
    "    y_pred=[]\n",
    "    y_true=[]\n",
    "    \n",
    "    t_start = time.time()\n",
    "    gnn_model.train()        \n",
    "    \n",
    "    edges, non_edges = sample_equal_number_edges_non_edges(adj_train_corrupted, false_non_edges=train_false_non_edges, false_edges=train_false_edges, small_samples=small_samples)\n",
    "    \n",
    "    samples = torch.cat((torch.Tensor(edges), torch.Tensor(non_edges)),dim=0).type(torch.long).to(device)\n",
    "    target = torch.cat((torch.ones(len(edges)), torch.zeros(len(non_edges))),dim=0).type(torch.long).to(device)\n",
    "    \n",
    "   \n",
    "    if minibatch_size > 0:\n",
    "        train_batcher = MiniBatcher(minibatch_size, len(samples))\n",
    "    else:\n",
    "        train_batcher = MiniBatcher(len(samples), len(samples))\n",
    "\n",
    "    for idxs in train_batcher.get_one_batch():\n",
    "        idxs = idxs.to(device)        \n",
    "        train_edges=samples[idxs]\n",
    "        train_target=target[idxs]        \n",
    "        nodex=torch.flatten(train_edges)  \n",
    "        train_nodes=torch.unique(nodex.view(-1))\n",
    "        \n",
    "        #NewSubgraphMaker(Graph, edges, nodes)\n",
    "        \n",
    "        \n",
    "        #subgraphs=NewSubgraphMaker(G_train, train_edges, nodex) \n",
    "        Graph=G_train        \n",
    "        neighbor=NeighborSamplerNew(Graph)             \n",
    "        graphGenerator=neighbor(nodex.cpu())    \n",
    "        subgraphs=[]\n",
    "        for graph in graphGenerator:        \n",
    "            sub_feature=torch.Tensor(Graph.x[graph.n_id])      \n",
    "            x_feature=[]        \n",
    "            for i in range(2):\n",
    "                unique_feature=torch.zeros(len(sub_feature),1)\n",
    "                for i_d in range(len(graph.n_id)):\n",
    "                        if graph.b_id[i]==graph.n_id[i_d]:\n",
    "                            pointu=i_d\n",
    "                            break;           \n",
    "                unique_feature[pointu]=1                        \n",
    "                x_feature.append(torch.cat((sub_feature,unique_feature),dim=1).to(device))\n",
    "            graph.x_feature=x_feature\n",
    "            subgraphs.append(graph.to(device))   \n",
    "        \n",
    "        gnn_optimizer.zero_grad()                \n",
    "        loss, pred=gnn_model.predict(subgraphs, train_nodes, train_edges, train_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        gnn_optimizer.step()\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "            \n",
    "        pred = F.log_softmax(pred, dim=1)\n",
    "        pred = pred.detach().to(\"cpu\").numpy()\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "\n",
    "        y_pred = np.append(y_pred,pred)\n",
    "        y_true = np.append(y_true,train_target.detach().to(\"cpu\").numpy())\n",
    "        \n",
    "    \n",
    "    t_end= time.time()\n",
    "    print(\"Minibatch time: \",t_end-t_start)\n",
    "    \n",
    "    train_acc=accuracy_score(y_true, y_pred)\n",
    "    tAccurs.append(train_acc)\n",
    "    tLosses.append(train_loss)\n",
    "    #predict_model(select, G_data, minibatch_size, small_samples):\n",
    "    v_loss,v_acc,_,_=predict_model(\"val\")\n",
    "    vLosses.append(v_loss)\n",
    "    vAccurs.append(v_acc)\n",
    "    \n",
    "    if v_acc > validation_acc:\n",
    "            validation_acc = v_acc\n",
    "            torch.save(gnn_model.state_dict(), gnn_best_model)\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    print(\"[Epoch {0}] Train Loss: {1}, Train Accuracy: {2}, Val Loss {3}, Val Accuracy: {4}\".format(num_epoch, train_loss, train_acc, v_loss, v_acc))\n",
    "    \n",
    "torch.save(gnn_model.state_dict(), gnn_last_model)\n",
    "\n",
    "save_plot(tAccurs, vAccurs, name='Accuracy')\n",
    "save_plot(tLosses, vLosses, name='Loss')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass on the test graphs (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn_best_model = 'RP_best_model.model'\n",
    "#gnn_model = GnnNew().to(device)\n",
    "gnn_model.load_state_dict(torch.load('RP_best_model.model'))\n",
    "\n",
    "from statistics import mean, stdev\n",
    "run_count=12\n",
    "\n",
    "test_accs=[]\n",
    "test_micros=[]\n",
    "test_weighteds=[]\n",
    "\n",
    "for i in range(run_count):\n",
    "    #predict_model(select, G_data, minibatch_size, small_samples):\n",
    "    _,test_acc,test_micro,test_weighted=predict_model(\"test\")\n",
    "    test_accs.append(test_acc)\n",
    "    test_micros.append(test_micro)\n",
    "    test_weighteds.append(test_weighted)\n",
    "    \n",
    "print(\"{0} ({1})\".format(mean(test_micros), stdev(test_micros)))\n",
    "\n",
    "print(\"Test Micro F1 Score: \", mean(test_micros))\n",
    "print(\"Test Weighted F1 Score: \", mean(test_weighteds))\n",
    "print(\"Test Accuracy Score: \", mean(test_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass on the test graphs (last model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn_last_model = 'RP_last_model.model'\n",
    "gnn_model = GnnNew().to(device)\n",
    "gnn_model.load_state_dict(torch.load('RP_last_model.model'))\n",
    "\n",
    "from statistics import mean, stdev\n",
    "run_count=12\n",
    "\n",
    "test_accs=[]\n",
    "test_micros=[]\n",
    "test_weighteds=[]\n",
    "\n",
    "for i in range(run_count):\n",
    "    _,test_acc,test_micro,test_weighted=predict_model(\"test\")\n",
    "    test_accs.append(test_acc)\n",
    "    test_micros.append(test_micro)\n",
    "    test_weighteds.append(test_weighted)\n",
    "    \n",
    "print(\"{0} ({1})\".format(mean(test_micros), stdev(test_micros)))\n",
    "    \n",
    "print(\"Test Micro F1 Score: \", mean(test_micros))\n",
    "print(\"Test Weighted F1 Score: \", mean(test_weighteds))\n",
    "print(\"Test Accuracy Score: \", mean(test_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batcher = MiniBatcher(2, 10) if minibatch_size > 0 else MiniBatcher(10, 10)\n",
    "\n",
    "for i in range(3):\n",
    "    for train_idxs in train_batcher.get_one_batch():\n",
    "        print(train_idxs)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.Tensor([[1,2],[3,1],[4,5]])\n",
    "b=torch.unique(torch.flatten(a).view(-1))\n",
    "\n",
    "k=1\n",
    "A = kneighbors_graph(a, k, mode=\"connectivity\", metric=\"cosine\", include_self=False)        \n",
    "(u,v)=A.nonzero()\n",
    "print(u,v)\n",
    "\n",
    "a[a==1]=3\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(np.random.randint(2))\n",
    "    \n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_37",
   "language": "python",
   "name": "deep_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
