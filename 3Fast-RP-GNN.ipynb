{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast-RP-GNN, Relational Pooling, Mini-batching, Subgraph batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Reddit, PPI, Planetoid\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch_geometric.utils.repeat import repeat\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "import time\n",
    "from torch_cluster import neighbor_sampler\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import degree, segregate_self_loops\n",
    "from torch_geometric.utils.repeat import repeat\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeighborSamplerNew(object):\n",
    "    \n",
    "    def __init__(self, data, size=0, num_hops=2, batch_size=1, shuffle=False,\n",
    "                 drop_last=False, bipartite=False, add_self_loops=False,\n",
    "                 flow='source_to_target'):\n",
    "\n",
    "        self.data = data\n",
    "        self.size = repeat(k, hop)\n",
    "        self.num_hops = hop\n",
    "        self.batch_size = neighbor_minibatch_size\n",
    "        self.shuffle = False\n",
    "        self.drop_last = False\n",
    "        self.bipartite = False\n",
    "        self.add_self_loops = False\n",
    "        self.flow = 'source_to_target'\n",
    "\n",
    "        self.edge_index = data.edge_index\n",
    "        self.e_id = torch.arange(self.edge_index.size(1))\n",
    "        if bipartite and add_self_loops:\n",
    "            tmp = segregate_self_loops(self.edge_index, self.e_id)\n",
    "            self.edge_index, self.e_id, self.edge_index_loop = tmp[:3]\n",
    "            self.e_id_loop = self.e_id.new_full((data.num_nodes, ), -1)\n",
    "            self.e_id_loop[tmp[2][0]] = tmp[3]\n",
    "\n",
    "        assert flow in ['source_to_target', 'target_to_source']\n",
    "        self.i, self.j = (0, 1) if flow == 'target_to_source' else (1, 0)\n",
    "\n",
    "        edge_index_i, self.e_assoc = self.edge_index[self.i].sort()\n",
    "        self.edge_index_j = self.edge_index[self.j, self.e_assoc]\n",
    "        deg = degree(edge_index_i, data.num_nodes, dtype=torch.long)\n",
    "        self.cumdeg = torch.cat([deg.new_zeros(1), deg.cumsum(0)])\n",
    "\n",
    "        self.tmp = torch.empty(data.num_nodes, dtype=torch.long)\n",
    "        \n",
    "\n",
    "\n",
    "    def __get_batches__(self, subset=None):\n",
    "        r\"\"\"Returns a list of mini-batches from the initial nodes in\n",
    "        :obj:`subset`.\"\"\"\n",
    "\n",
    "        if subset is None and not self.shuffle:\n",
    "            subset = torch.arange(self.data.num_nodes, dtype=torch.long)\n",
    "        elif subset is None and self.shuffle:\n",
    "            subset = torch.randperm(self.data.num_nodes)\n",
    "        else:\n",
    "            if subset.dtype == torch.bool or subset.dtype == torch.uint8:\n",
    "                subset = subset.nonzero().view(-1)\n",
    "            if self.shuffle:\n",
    "                subset = subset[torch.randperm(subset.size(0))]\n",
    "\n",
    "        subsets = torch.split(subset, self.batch_size)\n",
    "        if self.drop_last and subsets[-1].size(0) < self.batch_size:\n",
    "            subsets = subsets[:-1]\n",
    "        assert len(subsets) > 0\n",
    "        return subsets\n",
    "\n",
    "  \n",
    "    def __produce_subgraph__(self, b_id):\n",
    "        r\"\"\"Produces a :obj:`Data` object holding the subgraph data for a given\n",
    "        mini-batch :obj:`b_id`.\"\"\"\n",
    "\n",
    "        n_ids = [b_id]\n",
    "        e_ids = []\n",
    "        edge_indices = []\n",
    "\n",
    "        for l in range(self.num_hops):\n",
    "            e_id = neighbor_sampler(n_ids[-1], self.cumdeg, self.size[l])\n",
    "            n_id = self.edge_index_j.index_select(0, e_id)\n",
    "            n_id = n_id.unique(sorted=False)\n",
    "            n_ids.append(n_id)\n",
    "            e_ids.append(self.e_assoc.index_select(0, e_id))\n",
    "            edge_index = self.data.edge_index.index_select(1, e_ids[-1])\n",
    "            edge_indices.append(edge_index)\n",
    "\n",
    "        n_id = torch.unique(torch.cat(n_ids, dim=0), sorted=False)\n",
    "        self.tmp[n_id] = torch.arange(n_id.size(0))\n",
    "        e_id = torch.cat(e_ids, dim=0)\n",
    "        edge_index = self.tmp[torch.cat(edge_indices, dim=1)]\n",
    "\n",
    "        num_nodes = n_id.size(0)\n",
    "        idx = edge_index[0] * num_nodes + edge_index[1]\n",
    "        idx, inv = idx.unique(sorted=False, return_inverse=True)\n",
    "        edge_index = torch.stack([idx / num_nodes, idx % num_nodes], dim=0)\n",
    "        e_id = e_id.new_zeros(edge_index.size(1)).scatter_(0, inv, e_id)\n",
    "\n",
    "        return Data(edge_index=edge_index, e_id=e_id, n_id=n_id, b_id=b_id,\n",
    "                    sub_b_id=self.tmp[b_id], num_nodes=num_nodes)\n",
    "\n",
    "    def __call__(self, subset=None):\n",
    "        r\"\"\"Returns a generator of :obj:`DataFlow` that iterates over the nodes\n",
    "        in :obj:`subset` in a mini-batch fashion.\n",
    "        Args:\n",
    "            subset (LongTensor or BoolTensor, optional): The initial nodes to\n",
    "                propagete messages to. If set to :obj:`None`, will iterate over\n",
    "                all nodes in the graph. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        if self.bipartite:\n",
    "            produce = self.__produce_bipartite_data_flow__\n",
    "        else:\n",
    "            produce = self.__produce_subgraph__\n",
    "\n",
    "        for n_id in self.__get_batches__(subset):\n",
    "            yield produce(n_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset, the type of prediction and the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cora'\n",
    "PREDICTION = 'link'\n",
    "RUN_COUNT = 1\n",
    "NUM_SAMPLES = 1\n",
    "PATH_TO_DATASETS_DIRECTORY = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'reddit': Reddit(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Reddit'),\n",
    "    'cora' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Cora/', name='Cora'),\n",
    "    'citeseer' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/CiteSeer/', name='CiteSeer'),\n",
    "    'pubmed' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/PubMed/', name='PubMed'),\n",
    "}\n",
    "dataset = datasets[DATASET]\n",
    "data = dataset[0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Dataset Characteristics\n",
      "Name:  cora\n",
      "Total Number of Nodes:  2708\n",
      "Total Number of Training Nodes:  140\n",
      "Total Number of Val Nodes:  500\n",
      "Total Number of Test Nodes:  1000\n",
      "Num Node Features:  1433\n",
      "Num Node Classes:  7\n",
      "Number of Edges:  10556\n",
      "Number of Samples for structural:  1\n",
      "Prediction Type:  link\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing Dataset Characteristics\")\n",
    "print(\"Name: \", DATASET)\n",
    "print(\"Total Number of Nodes: \", data.num_nodes)\n",
    "print(\"Total Number of Training Nodes: \", data.train_mask.sum().item())\n",
    "print(\"Total Number of Val Nodes: \", data.val_mask.sum().item())\n",
    "print(\"Total Number of Test Nodes: \", data.test_mask.sum().item())\n",
    "print(\"Num Node Features: \", data.num_features)\n",
    "print(\"Num Node Classes: \", dataset.num_classes)\n",
    "print(\"Number of Edges: \", data.edge_index.shape[1])\n",
    "print(\"Number of Samples for structural: \", NUM_SAMPLES)\n",
    "print(\"Prediction Type: \", PREDICTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.train_mask = 1 - data.val_mask - data.test_mask\n",
    "data.train_mask = ~(data.val_mask + data.test_mask)\n",
    "\n",
    "adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "edges = data.edge_index.t()\n",
    "adj_mat[edges[:,0], edges[:,1]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the non-overlapping induced subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train = adj_mat[data.train_mask].t()[data.train_mask].t()\n",
    "adj_validation = adj_mat[data.val_mask].t()[data.val_mask].t()\n",
    "adj_test = adj_mat[data.test_mask].t()[data.test_mask].t()\n",
    "\n",
    "\n",
    "GnnLayer=GATConv\n",
    "gnn_model = 0\n",
    "epochs = 100\n",
    "validation_acc = 0\n",
    "small_samples = 200\n",
    "\n",
    "minibatch_size=16\n",
    "neighbor_minibatch_size=2 #for current implementation use 2 for now\n",
    "k=5\n",
    "hop=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrupt a small fraction of the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_adj(adj_mat, task, percent=2):\n",
    "    \"\"\" Returns the corrupted version of the adjacency matrix \"\"\"\n",
    "    if task == 'link':\n",
    "        edges = adj_mat.triu().nonzero()\n",
    "        num_edges = edges.shape[0]\n",
    "        num_to_corrupt = int(percent/100.0 * num_edges)\n",
    "        random_corruption = np.random.randint(num_edges, size=num_to_corrupt)\n",
    "        adj_mat_corrupted = adj_mat.clone()\n",
    "        false_edges, false_non_edges = [], []\n",
    "        #Edge Corruption\n",
    "        for ed in edges[random_corruption]:\n",
    "            adj_mat_corrupted[ed[0], ed[1]] = 0\n",
    "            adj_mat_corrupted[ed[1], ed[0]] = 0\n",
    "            false_non_edges.append(ed.tolist())\n",
    "        #Non Edge Corruption\n",
    "        random_non_edge_corruption = list(np.random.randint(adj_mat.shape[0], size = 6*num_to_corrupt))\n",
    "        non_edge_to_corrupt = []\n",
    "        for k in range(len(random_non_edge_corruption)-1):\n",
    "            to_check = [random_non_edge_corruption[k], random_non_edge_corruption[k+1]]\n",
    "            if to_check not in edges.tolist():\n",
    "                non_edge_to_corrupt.append(to_check)\n",
    "            if len(non_edge_to_corrupt) == num_to_corrupt:\n",
    "                break\n",
    "        non_edge_to_corrupt = torch.Tensor(non_edge_to_corrupt).type(torch.int16)\n",
    "        for n_ed in non_edge_to_corrupt:\n",
    "            adj_mat_corrupted[n_ed[0], n_ed[1]] = 1\n",
    "            adj_mat_corrupted[n_ed[1], n_ed[0]] = 1\n",
    "            false_edges.append(n_ed.tolist())\n",
    "    return adj_mat_corrupted, false_edges, false_non_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train edge index: tensor([[   1,    1,    1,  ..., 1205, 1205, 1207],\n",
      "        [   2,  152,  154,  ..., 1112, 1124,  941]])\n"
     ]
    }
   ],
   "source": [
    "adj_train_corrupted, train_false_edges, train_false_non_edges = corrupt_adj(adj_train, 'link', percent=2)\n",
    "adj_val_corrupted, val_false_edges, val_false_non_edges = corrupt_adj(adj_validation, 'link', percent=2)\n",
    "adj_test_corrupted, test_false_edges, test_false_non_edges  = corrupt_adj(adj_test, 'link', percent=2)\n",
    "\n",
    "G_train=Data(edge_index=(adj_train_corrupted.nonzero()).t(), x=data.x[data.train_mask])\n",
    "print(\"train edge index:\", G_train.edge_index)\n",
    "G_val=Data(edge_index=(adj_val_corrupted.nonzero()).t(), x=data.x[data.val_mask])\n",
    "G_test=Data(edge_index=(adj_test_corrupted.nonzero()).t(), x=data.x[data.test_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the GNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nneurons = 256\n",
    "Input_Dim_rep = data.num_features + 1 #aditional feature for expressiveness of the Graph Convolution\n",
    "\n",
    "class GnnNew(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GnnNew, self).__init__()        \n",
    "     \n",
    "        if(GnnLayer==GINConv):\n",
    "            self.MLP1 = nn.Linear(Input_Dim_rep,Nneurons)\n",
    "            self.MLP2 = nn.Linear(Nneurons,Nneurons)\n",
    "            self.GNN_layer1 = GnnLayer(self.MLP1)\n",
    "            self.GNN_layer2 = GnnLayer(self.MLP2)  \n",
    "        \n",
    "        else:\n",
    "            self.GNN_layer1 = GnnLayer(Input_Dim_rep,Nneurons)\n",
    "            self.GNN_layer2 = GnnLayer(Nneurons,Nneurons)             \n",
    "    \n",
    "        self.ds_layer_1 = nn.Linear(Nneurons, Nneurons)\n",
    "        self.ro_1 = nn.Linear(Nneurons, Nneurons)      \n",
    "              \n",
    "        self.linear_1 = nn.Linear(Nneurons,Nneurons)\n",
    "        self.linear_2 = nn.Linear(Nneurons,2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, subgraphs, nodes, edges):        \n",
    "        \n",
    "        \n",
    "        countNode = torch.zeros((len(nodes)),requires_grad=False).to(device)\n",
    "        tensorNode = torch.zeros((len(nodes), Nneurons)).to(device)\n",
    "        \n",
    "        for i in range(len(subgraphs)):\n",
    "        \n",
    "            graph=subgraphs[i]\n",
    "            feat=graph.x_feature\n",
    "            lenFeat=len(feat)\n",
    "            ed_i=graph.edge_index\n",
    "            Nfeat=feat[0].shape[0]\n",
    "            \n",
    "            Ysum = torch.zeros((Nfeat, Nneurons)).to(device)\n",
    "            \n",
    "            for j in range(lenFeat):           \n",
    "      \n",
    "                y=self.GNN_layer1(feat[j],ed_i)\n",
    "                y=self.relu(y)\n",
    "\n",
    "                y=self.GNN_layer2(y,ed_i)\n",
    "                y=self.relu(y)       \n",
    "                Ysum=Ysum+y\n",
    "            \n",
    "            Yavg=Ysum/lenFeat\n",
    "            \n",
    "            for node_i_d in graph.b_id:             \n",
    "                # b_id holds a one-dimensional tensor of node indices to produce subgraphs \n",
    "                #for and e_id and n_id hold the indices that got samples from the original graph for edges \n",
    "                #and nodes respectively. Since you are having a batch size of 1, b_id should be of size 1.\n",
    "#                 print(\"graph.nid\", graph.n_id)\n",
    "#                 print(\"nodeId\", node_i_d)\n",
    "#                 print(\"graph.nid=nodeId\", (graph.n_id==node_i_d))\n",
    "#                 print(\"subindex\", (graph.n_id==node_i_d).nonzero()[0])\n",
    "                sub_index=0\n",
    "                for i_d in range(len(graph.n_id)):\n",
    "                    if graph.n_id[i_d]==node_i_d:\n",
    "                        sub_index=i_d\n",
    "                        break;\n",
    "                #print(\"subindexnew\", sub_index)\n",
    "                #sub_index=(graph.n_id==node_i_d).nonzero()[0]\n",
    "                sub_x=Yavg[sub_index,:]\n",
    "                node_index=0\n",
    "                for i_d in range(len(nodes)):\n",
    "                    if nodes[i_d]==node_i_d:\n",
    "                        node_index=i_d\n",
    "                        break;\n",
    "                \n",
    "                #node_index=(nodes==node_i_d).nonzero()[0]\n",
    "#                 print(\"node.nid\", nodes)\n",
    "#                 print(\"nodeId\", node_i_d)\n",
    "#                 print(\"nodes=nodeId\", (nodes==node_i_d))\n",
    "#                 print(\"nodeindex\", (nodes==node_i_d).nonzero()[0], \"actual Nid: \",node_index)                \n",
    "#                 print(sub_x.shape)\n",
    "#                 print(sub_x.view(-1).shape)\n",
    "                tensorNode[node_index,:]+=sub_x.view(-1)\n",
    "                countNode[node_index]+=1\n",
    "#         print(tensorNode)\n",
    "#         print(countNode)\n",
    "#         print(countNode[:, None])\n",
    "        #print(countNode.unsqueeze(0))\n",
    "        #print(countNode.unsqueeze(1))\n",
    "       \n",
    "        #tensorNode=tensorNode / countNode[:, None] \n",
    "        linkN=0\n",
    "        returnTensor = torch.zeros((len(edges), Nneurons)).to(device)\n",
    "        #print(\"retTensor\", returnTensor)\n",
    "        \n",
    "        tensorNode=tensorNode / countNode.unsqueeze(1)\n",
    "        for e in edges:\n",
    "            for i_d in range(len(nodes)):\n",
    "                    if nodes[i_d]==e[0]:\n",
    "                        point1=i_d\n",
    "                        break;\n",
    "            for i_d in range(len(nodes)):\n",
    "                    if nodes[i_d]==e[1]:\n",
    "                        point2=i_d\n",
    "                        break;\n",
    "#             u=(nodes==e[0]).nonzero()[0]\n",
    "#             v=(nodes==e[1]).nonzero()[0]\n",
    "  #          print(\"uv\",u,v,point1,point2)\n",
    "            u=tensorNode[point1,:]                        \n",
    "            v=tensorNode[point2,:]\n",
    "                        \n",
    "            ##we can use pooling                                    \n",
    "            u_v=u+v            \n",
    "            returnTensor[linkN,:]=u_v\n",
    "            linkN+=1\n",
    "        \n",
    "        \n",
    "        #One Hidden Layer for predictor        \n",
    "        returnTensor = self.linear_1(returnTensor)\n",
    "        returnTensor = self.relu(returnTensor)\n",
    "        returnTensor = self.linear_2(returnTensor)\n",
    "        \n",
    "        return returnTensor\n",
    "\n",
    "    def compute_loss(self, sub_graphs, nodes, edges, target):\n",
    "        \n",
    "        pred = self.forward(sub_graphs, nodes, edges)        \n",
    "        loss = F.cross_entropy(pred, target)                \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, sub_graphs, nodes, edges, target):\n",
    "        \n",
    "        pred = self.forward(sub_graphs, nodes, edges)        \n",
    "        loss = F.cross_entropy(pred, target)\n",
    "        \n",
    "        return loss, pred\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample positive and negative edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_equal_number_edges_non_edges(adj_mat, false_non_edges, false_edges, small_samples):\n",
    "    edges = adj_mat.nonzero()\n",
    "    num_edges = edges.shape[0]\n",
    "    inverse_adj_mat = 1 - adj_mat\n",
    "    non_edges = inverse_adj_mat.nonzero()\n",
    "    num_non_edges  = non_edges.shape[0]\n",
    "    \n",
    "    edges_sampled = edges[np.random.randint(num_edges, size=small_samples)]\n",
    "    non_edges_sampled = non_edges[np.random.randint(num_non_edges, size=small_samples)]\n",
    "    final_edges = []\n",
    "    final_non_edges = []\n",
    "    for ed in edges_sampled.tolist():\n",
    "        if ed not in false_edges:\n",
    "            final_edges.append(ed)\n",
    "    final_edges += false_non_edges\n",
    "    for n_ed in non_edges_sampled.tolist():\n",
    "        if n_ed not in false_non_edges:\n",
    "            final_non_edges.append(n_ed)\n",
    "    final_non_edges += false_edges\n",
    "\n",
    "    return final_edges, final_non_edges\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "def sample_train_edges_nearest_neighbor(train_feature, adj_mat, false_non_edges, false_edges, small_samples):\n",
    "    edges = adj_mat.nonzero()\n",
    "    num_edges = edges.shape[0]\n",
    "    \n",
    "    k=5\n",
    "    A = kneighbors_graph(train_feature, k, mode=\"connectivity\", metric=\"cosine\", include_self=False)        \n",
    "    (u,v)=A.nonzero()\n",
    "    u=torch.Tensor(u).type(torch.long)\n",
    "    v=torch.Tensor(v).type(torch.long)\n",
    "    possible_edges=torch.stack((u,v),dim=1)\n",
    "    possible_adj_mat = torch.zeros((adj_mat.shape))\n",
    "    possible_adj_mat[possible_edges[:,0], possible_edges[:,1]] = 1\n",
    "    \n",
    "    inverse_adj_mat=possible_adj_mat-adj_mat\n",
    "    inverse_adj_mat[inverse_adj_mat==-1]=0\n",
    "    non_edges = inverse_adj_mat.nonzero()  \n",
    "    \n",
    "    print(non_edges.shape)\n",
    "    \n",
    "    num_non_edges  = non_edges.shape[0]\n",
    "    edges_sampled = edges[np.random.randint(num_edges, size=small_samples)]\n",
    "    non_edges_sampled = non_edges[np.random.randint(num_non_edges, size=small_samples)]\n",
    "    final_edges = []\n",
    "    final_non_edges = []\n",
    "    for ed in edges_sampled.tolist():\n",
    "        if ed not in false_edges:\n",
    "            final_edges.append(ed)\n",
    "    final_edges += false_non_edges\n",
    "    for n_ed in non_edges_sampled.tolist():\n",
    "        if n_ed not in false_non_edges:\n",
    "            final_non_edges.append(n_ed)\n",
    "    final_non_edges += false_edges\n",
    "\n",
    "    return final_edges, final_non_edges\n",
    "\n",
    "# edges, non_edges = sample_equal_number_edges_non_edges(adj_train_corrupted, false_non_edges=train_false_non_edges, false_edges=train_false_edges, small_samples=small_samples)\n",
    "# edges, non_edges = sample_train_edges_nearest_neighbor(G_train.x, adj_train_corrupted, false_non_edges=train_false_non_edges, false_edges=train_false_edges, small_samples=small_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatcher(object):\n",
    "    def __init__(self, batch_size, n_examples, shuffle=True):\n",
    "        assert batch_size <= n_examples, \"Error: batch_size is larger than n_examples\"\n",
    "        self.batch_size = batch_size\n",
    "        self.n_examples = n_examples\n",
    "        self.shuffle = shuffle\n",
    "        logging.info(\"batch_size={}, n_examples={}\".format(batch_size, n_examples))\n",
    "\n",
    "        self.idxs = np.arange(self.n_examples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.current_start = 0\n",
    "\n",
    "    def get_one_batch(self):\n",
    "        self.idxs = np.arange(self.n_examples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.current_start = 0\n",
    "        while self.current_start < self.n_examples:\n",
    "            batch_idxs = self.idxs[self.current_start:self.current_start+self.batch_size]\n",
    "            self.current_start += self.batch_size\n",
    "            yield torch.LongTensor(batch_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewSubgraphMaker(Graph, edges, nodes, neighbor):\n",
    "    \n",
    "    #nodes=torch.flatten(edges)            \n",
    "    graphGenerator=neighbor(nodes.cpu())\n",
    "    \n",
    "    subgraphs=[]\n",
    "\n",
    "    for graph in graphGenerator:\n",
    "        \n",
    "        sub_feature=torch.Tensor(Graph.x[graph.n_id])      \n",
    "        x_feature=[]                   \n",
    "        #print(\"graphNid\", graph.n_id)\n",
    "        ##different permutation of for NewX=X*[one_hot_color]\n",
    "        for i in range(2):\n",
    "            unique_feature=torch.zeros(len(sub_feature),1)\n",
    "            for i_d in range(len(graph.n_id)):\n",
    "                    if graph.b_id[i]==graph.n_id[i_d]:\n",
    "                        pointu=i_d\n",
    "                        break;\n",
    "            #print(\"graphBid\", graph.b_id[i])\n",
    "            #u=(graph.n_id==graph.b_id[i]).nonzero()[0]\n",
    "            #print(graph.n_id==graph.b_id[i])\n",
    "            #print(\"u and pointu:\", u,pointu)\n",
    "            unique_feature[pointu]=1                        \n",
    "            x_feature.append(torch.cat((sub_feature,unique_feature),dim=1).to(device))\n",
    "\n",
    "        graph.x_feature=x_feature\n",
    "        subgraphs.append(graph.to(device))\n",
    "        \n",
    "    return subgraphs\n",
    "\n",
    "def predict_model(G_data, neighbor_sampler, minibatch_size, small_samples, adj_corrupted, false_non_edges, false_edges):\n",
    "    gnn_model.eval()\n",
    "    preds=np.array([])\n",
    "    targets=np.array([])\n",
    "    total_loss=0\n",
    "    \n",
    "    edges, non_edges = sample_equal_number_edges_non_edges(adj_corrupted, false_non_edges, false_edges, small_samples)    \n",
    "    samples = torch.cat((torch.Tensor(edges), torch.Tensor(non_edges)),dim=0).type(torch.long).to(device)\n",
    "    true_target = torch.cat((torch.ones(len(edges)), torch.zeros(len(non_edges))),dim=0).type(torch.long).to(device)\n",
    "    \n",
    "    batcher = MiniBatcher(minibatch_size, len(samples)) if minibatch_size > 0 else MiniBatcher(len(samples), len(samples))\n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_idxs in batcher.get_one_batch():\n",
    "            x_idxs = x_idxs.to(device)\n",
    "\n",
    "            x_edges=samples[x_idxs]\n",
    "            y_target=true_target[x_idxs]\n",
    "            \n",
    "            nodex=torch.flatten(x_edges)            \n",
    "            \n",
    "            nodes=torch.unique(nodex.view(-1))\n",
    "\n",
    "            subgraphs=NewSubgraphMaker(G_data, x_edges, nodex, neighbor_sampler)\n",
    "            \n",
    "            loss, pred=gnn_model.predict(subgraphs, nodes, x_edges, y_target)                          \n",
    "            total_loss+=loss.item()\n",
    "            \n",
    "            pred = F.log_softmax(pred, dim=1)\n",
    "            pred = pred.detach().to(\"cpu\").numpy()\n",
    "            pred = np.argmax(pred, axis=1)\n",
    "                          \n",
    "            preds = np.append(preds,pred)\n",
    "            targets = np.append(targets,y_target.detach().to(\"cpu\").numpy())\n",
    "        \n",
    "\n",
    "    micro=f1_score(targets, preds, average='micro')\n",
    "    weighted=f1_score(targets, preds, average='weighted')\n",
    "    acc=accuracy_score(targets, preds)\n",
    "    \n",
    "    return total_loss, acc, micro, weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Accuracy and Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(train_data, valid_data, name='Loss'):\n",
    "    \"\"\"Plot\n",
    "        Plot one figure: accurace/loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_data)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_data, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, valid_data, '-', linewidth=2, label='valid')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(name)\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    plt.savefig('train_valid_'+name+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch time:  8.817728042602539\n",
      "Saving model Validation accuracy:  0.5246305418719212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 0] Train Loss: 19.46040391921997, Train Accuracy: 0.527027027027027, Val Loss 17.55571937561035, Val Accuracy: 0.5246305418719212\n",
      "Minibatch time:  8.688697576522827\n",
      "Saving model Validation accuracy:  0.5724815724815725\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 1] Train Loss: 18.76618206501007, Train Accuracy: 0.6081081081081081, Val Loss 17.325248301029205, Val Accuracy: 0.5724815724815725\n",
      "Minibatch time:  8.927340507507324\n",
      "Saving model Validation accuracy:  0.6732186732186732\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 2] Train Loss: 17.542989015579224, Train Accuracy: 0.6561085972850679, Val Loss 16.27724277973175, Val Accuracy: 0.6732186732186732\n",
      "Minibatch time:  8.595733165740967\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 3] Train Loss: 17.65963351726532, Train Accuracy: 0.6794582392776524, Val Loss 16.176501154899597, Val Accuracy: 0.6535626535626535\n",
      "Minibatch time:  8.52572512626648\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 4] Train Loss: 16.228149950504303, Train Accuracy: 0.701123595505618, Val Loss 15.760820657014847, Val Accuracy: 0.6502463054187192\n",
      "Minibatch time:  8.365084171295166\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 5] Train Loss: 15.278014332056046, Train Accuracy: 0.7387387387387387, Val Loss 16.022075355052948, Val Accuracy: 0.6519607843137255\n",
      "Minibatch time:  8.411593198776245\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 6] Train Loss: 14.293206363916397, Train Accuracy: 0.7697516930022573, Val Loss 17.32730758190155, Val Accuracy: 0.5503685503685504\n",
      "Minibatch time:  8.703536987304688\n",
      "Saving model Validation accuracy:  0.6805896805896806\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 7] Train Loss: 14.958909094333649, Train Accuracy: 0.7522522522522522, Val Loss 15.656748622655869, Val Accuracy: 0.6805896805896806\n",
      "Minibatch time:  10.145098209381104\n",
      "Saving model Validation accuracy:  0.6987654320987654\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 8] Train Loss: 14.51421195268631, Train Accuracy: 0.7847533632286996, Val Loss 14.91244238615036, Val Accuracy: 0.6987654320987654\n",
      "Minibatch time:  8.789616107940674\n",
      "Saving model Validation accuracy:  0.7413793103448276\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 9] Train Loss: 13.834049239754677, Train Accuracy: 0.777027027027027, Val Loss 13.451230615377426, Val Accuracy: 0.7413793103448276\n",
      "Minibatch time:  8.8589026927948\n",
      "Saving model Validation accuracy:  0.773955773955774\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 10] Train Loss: 13.034937262535095, Train Accuracy: 0.7878103837471784, Val Loss 13.854390174150467, Val Accuracy: 0.773955773955774\n",
      "Minibatch time:  8.887789726257324\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 11] Train Loss: 13.89018577337265, Train Accuracy: 0.7692307692307693, Val Loss 15.723877608776093, Val Accuracy: 0.6345679012345679\n",
      "Minibatch time:  8.56937551498413\n",
      "Saving model Validation accuracy:  0.7911547911547911\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 12] Train Loss: 11.173478364944458, Train Accuracy: 0.8198198198198198, Val Loss 12.218638986349106, Val Accuracy: 0.7911547911547911\n",
      "Minibatch time:  8.652724981307983\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 13] Train Loss: 12.571670830249786, Train Accuracy: 0.8081264108352144, Val Loss 13.018492341041565, Val Accuracy: 0.7616707616707616\n",
      "Minibatch time:  8.533885478973389\n",
      "Saving model Validation accuracy:  0.8676470588235294\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 14] Train Loss: 9.393350586295128, Train Accuracy: 0.8853932584269663, Val Loss 8.671202033758163, Val Accuracy: 0.8676470588235294\n",
      "Minibatch time:  8.918176412582397\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 15] Train Loss: 8.130930922925472, Train Accuracy: 0.8963963963963963, Val Loss 9.430390983819962, Val Accuracy: 0.8469135802469135\n",
      "Minibatch time:  8.883864879608154\n",
      "Saving model Validation accuracy:  0.941031941031941\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 16] Train Loss: 5.253878904506564, Train Accuracy: 0.9253393665158371, Val Loss 4.710161432623863, Val Accuracy: 0.941031941031941\n",
      "Minibatch time:  8.824998378753662\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 17] Train Loss: 5.407566616311669, Train Accuracy: 0.9346846846846847, Val Loss 4.591455016285181, Val Accuracy: 0.9333333333333333\n",
      "Minibatch time:  10.484084367752075\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 18] Train Loss: 5.474752135574818, Train Accuracy: 0.9258426966292135, Val Loss 5.286607375368476, Val Accuracy: 0.9262899262899262\n",
      "Minibatch time:  8.449105262756348\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 19] Train Loss: 4.403996522538364, Train Accuracy: 0.9411764705882353, Val Loss 5.1354435086250305, Val Accuracy: 0.9164619164619164\n",
      "Minibatch time:  8.418923377990723\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 20] Train Loss: 5.023954812437296, Train Accuracy: 0.9459459459459459, Val Loss 5.087001932784915, Val Accuracy: 0.9333333333333333\n",
      "Minibatch time:  8.35115933418274\n",
      "Saving model Validation accuracy:  0.9459459459459459\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 21] Train Loss: 2.7905634976923466, Train Accuracy: 0.9708520179372198, Val Loss 4.893089160323143, Val Accuracy: 0.9459459459459459\n",
      "Minibatch time:  8.841652870178223\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 22] Train Loss: 2.8686247868463397, Train Accuracy: 0.9684684684684685, Val Loss 4.733116544783115, Val Accuracy: 0.9333333333333333\n",
      "Minibatch time:  8.452003479003906\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 23] Train Loss: 2.0495110861957073, Train Accuracy: 0.9819004524886877, Val Loss 5.489109800662845, Val Accuracy: 0.9238329238329238\n",
      "Minibatch time:  8.827500104904175\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 24] Train Loss: 3.266961561399512, Train Accuracy: 0.9640449438202248, Val Loss 4.267977377399802, Val Accuracy: 0.9433497536945813\n",
      "Minibatch time:  8.644936084747314\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 25] Train Loss: 1.229097575880587, Train Accuracy: 0.990990990990991, Val Loss 4.8838624979835, Val Accuracy: 0.9384236453201971\n",
      "Minibatch time:  8.559515714645386\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 26] Train Loss: 2.2876617573201656, Train Accuracy: 0.9796839729119639, Val Loss 5.323609607759863, Val Accuracy: 0.9334975369458128\n",
      "Minibatch time:  8.489564418792725\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 27] Train Loss: 2.2976044504903257, Train Accuracy: 0.963963963963964, Val Loss 5.688608878757805, Val Accuracy: 0.9432098765432099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch time:  10.485574007034302\n",
      "Saving model Validation accuracy:  0.948019801980198\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 28] Train Loss: 3.051232214551419, Train Accuracy: 0.9547511312217195, Val Loss 3.9625163502059877, Val Accuracy: 0.948019801980198\n",
      "Minibatch time:  10.96414303779602\n",
      "Saving model Validation accuracy:  0.9529702970297029\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 29] Train Loss: 1.2648123267572373, Train Accuracy: 0.9864864864864865, Val Loss 4.332603582646698, Val Accuracy: 0.9529702970297029\n",
      "Minibatch time:  9.30375337600708\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 30] Train Loss: 2.7822734157089144, Train Accuracy: 0.9730337078651685, Val Loss 5.117241688072681, Val Accuracy: 0.9407407407407408\n",
      "Minibatch time:  9.290193557739258\n",
      "Saving model Validation accuracy:  0.9534313725490197\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 31] Train Loss: 1.797746012918651, Train Accuracy: 0.9775784753363229, Val Loss 3.8905197172425687, Val Accuracy: 0.9534313725490197\n",
      "Minibatch time:  10.618908643722534\n",
      "Saving model Validation accuracy:  0.9554455445544554\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 32] Train Loss: 1.988234435673803, Train Accuracy: 0.9797752808988764, Val Loss 4.107179756741971, Val Accuracy: 0.9554455445544554\n",
      "Minibatch time:  9.057308673858643\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 33] Train Loss: 1.2805942450650036, Train Accuracy: 0.9887133182844243, Val Loss 6.455067239468917, Val Accuracy: 0.9384236453201971\n",
      "Minibatch time:  8.841383695602417\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 34] Train Loss: 2.14039449125994, Train Accuracy: 0.9820627802690582, Val Loss 7.157588450005278, Val Accuracy: 0.9234567901234568\n",
      "Minibatch time:  9.126347303390503\n",
      "Saving model Validation accuracy:  0.9556650246305419\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 35] Train Loss: 1.7365845820168033, Train Accuracy: 0.9774774774774775, Val Loss 4.705238507594913, Val Accuracy: 0.9556650246305419\n",
      "Minibatch time:  9.962828397750854\n",
      "Saving model Validation accuracy:  0.9606879606879607\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Epoch 36] Train Loss: 1.289845969993621, Train Accuracy: 0.9820224719101124, Val Loss 4.229710875544697, Val Accuracy: 0.9606879606879607\n"
     ]
    }
   ],
   "source": [
    "gnn_model = GnnNew().to(device)\n",
    "gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "gnn_best_model = '3best_gnn_inductive_model.model'\n",
    "gnn_last_model = '3last_gnn_inductive_model.model'\n",
    "\n",
    "\n",
    "\n",
    "train_neighbor_sampler=NeighborSamplerNew(G_train)\n",
    "val_neighbor_sampler=NeighborSamplerNew(G_val)\n",
    "test_neighbor_sampler=NeighborSamplerNew(G_test)\n",
    "\n",
    "\n",
    "train_accs=[]\n",
    "val_accs=[]\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "    gnn_model.train()        \n",
    "    \n",
    "    edges, non_edges = sample_equal_number_edges_non_edges(adj_train_corrupted, false_non_edges=train_false_non_edges, false_edges=train_false_edges, small_samples=small_samples)\n",
    "    #edges, non_edges = sample_train_edges_nearest_neighbor(G_train.x, adj_train_corrupted, false_non_edges=train_false_non_edges, false_edges=train_false_edges, small_samples=small_samples)\n",
    "    \n",
    "    samples = torch.cat((torch.Tensor(edges), torch.Tensor(non_edges)),dim=0).type(torch.long).to(device)\n",
    "    target = torch.cat((torch.ones(len(edges)), torch.zeros(len(non_edges))),dim=0).type(torch.long).to(device)\n",
    "    \n",
    "    \n",
    "    train_batcher = MiniBatcher(minibatch_size, len(samples)) if minibatch_size > 0 else MiniBatcher(len(samples), len(samples))\n",
    "    \n",
    "    train_loss=0\n",
    "    y_pred=[]\n",
    "    y_true=[]\n",
    "    \n",
    "    t_start = time.time()\n",
    "    for train_idxs in train_batcher.get_one_batch():\n",
    "        train_idxs = train_idxs.to(device)\n",
    "        \n",
    "        train_edges=samples[train_idxs]\n",
    "        train_target=target[train_idxs]\n",
    "        \n",
    "        nodex=torch.flatten(train_edges)  \n",
    "        train_nodes=torch.unique(nodex.view(-1))\n",
    "\n",
    "        subgraphs=NewSubgraphMaker(G_train, train_edges, nodex, train_neighbor_sampler)\n",
    "        \n",
    "        \n",
    "        \n",
    "        gnn_optimizer.zero_grad()        \n",
    "                \n",
    "        loss, pred=gnn_model.predict(subgraphs, train_nodes, train_edges, train_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        gnn_optimizer.step()\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "            \n",
    "        pred = F.log_softmax(pred, dim=1)\n",
    "        pred = pred.detach().to(\"cpu\").numpy()\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "\n",
    "        y_pred = np.append(y_pred,pred)\n",
    "        y_true = np.append(y_true,train_target.detach().to(\"cpu\").numpy())\n",
    "        \n",
    "    \n",
    "    t_end= time.time()\n",
    "    print(\"Minibatch time: \",t_end-t_start)\n",
    "    \n",
    "    train_acc=accuracy_score(y_true, y_pred)\n",
    "    train_accs.append(train_acc)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_loss,val_acc,_,_=predict_model(G_val, val_neighbor_sampler, minibatch_size, small_samples, adj_val_corrupted, val_false_non_edges, val_false_edges)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if val_acc > validation_acc:\n",
    "            validation_acc = val_acc\n",
    "            print(\"Saving model Validation accuracy: \", validation_acc)\n",
    "            #Save Model\n",
    "            torch.save(gnn_model.state_dict(), gnn_best_model)\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    print(\"[Epoch {0}] Train Loss: {1}, Train Accuracy: {2}, Val Loss {3}, Val Accuracy: {4}\".format(num_epoch, train_loss, train_acc, val_loss, val_acc))\n",
    "    \n",
    "torch.save(gnn_model.state_dict(), gnn_last_model)\n",
    "\n",
    "save_plot(train_accs, val_accs, name='Accuracy')\n",
    "save_plot(train_losses, val_losses, name='Loss')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass on the test graphs (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_best_model = '3best_gnn_inductive_model.model'\n",
    "#gnn_model = GnnNew().to(device)\n",
    "gnn_model.load_state_dict(torch.load(gnn_best_model))\n",
    "\n",
    "from statistics import mean, stdev\n",
    "run_count=12\n",
    "\n",
    "test_accs=[]\n",
    "test_micros=[]\n",
    "test_weighteds=[]\n",
    "\n",
    "for i in range(run_count):\n",
    "    _,test_acc,test_micro,test_weighted=predict_model(G_test, test_neighbor_sampler, minibatch_size, small_samples, adj_test_corrupted, test_false_non_edges, test_false_edges)\n",
    "    test_accs.append(test_acc)\n",
    "    test_micros.append(test_micro)\n",
    "    test_weighteds.append(test_weighted)\n",
    "    \n",
    "print(\"{0} ({1})\".format(mean(test_micros), stdev(test_micros)))\n",
    "\n",
    "print(\"Test Micro F1 Score: \", mean(test_micros))\n",
    "print(\"Test Weighted F1 Score: \", mean(test_weighteds))\n",
    "print(\"Test Accuracy Score: \", mean(test_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass on the test graphs (last model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_last_model = '3last_gnn_inductive_model.model'\n",
    "gnn_model = GnnNew().to(device)\n",
    "gnn_model.load_state_dict(torch.load(gnn_last_model))\n",
    "\n",
    "from statistics import mean, stdev\n",
    "run_count=12\n",
    "\n",
    "test_accs=[]\n",
    "test_micros=[]\n",
    "test_weighteds=[]\n",
    "\n",
    "for i in range(run_count):\n",
    "    _,test_acc,test_micro,test_weighted=predict_model(G_test, test_neighbor_sampler, minibatch_size, small_samples, adj_test_corrupted, test_false_non_edges, test_false_edges)\n",
    "    test_accs.append(test_acc)\n",
    "    test_micros.append(test_micro)\n",
    "    test_weighteds.append(test_weighted)\n",
    "    \n",
    "print(\"{0} ({1})\".format(mean(test_micros), stdev(test_micros)))\n",
    "    \n",
    "print(\"Test Micro F1 Score: \", mean(test_micros))\n",
    "print(\"Test Weighted F1 Score: \", mean(test_weighteds))\n",
    "print(\"Test Accuracy Score: \", mean(test_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batcher = MiniBatcher(2, 10) if minibatch_size > 0 else MiniBatcher(10, 10)\n",
    "\n",
    "for i in range(3):\n",
    "    for train_idxs in train_batcher.get_one_batch():\n",
    "        print(train_idxs)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.Tensor([[1,2],[3,1],[4,5]])\n",
    "b=torch.unique(torch.flatten(a).view(-1))\n",
    "\n",
    "k=1\n",
    "A = kneighbors_graph(a, k, mode=\"connectivity\", metric=\"cosine\", include_self=False)        \n",
    "(u,v)=A.nonzero()\n",
    "print(u,v)\n",
    "\n",
    "a[a==1]=3\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(np.random.randint(2))\n",
    "    \n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_37",
   "language": "python",
   "name": "deep_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
